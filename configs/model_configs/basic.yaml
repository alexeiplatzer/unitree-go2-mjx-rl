model:
  model_class: ActorCritic
  policy:
    layer_sizes:
    - 128
    - 128
    - 128
    - 128
    - 128
  policy_obs_key: proprioceptive
  value:
    layer_sizes:
    - 256
    - 256
    - 256
    - 256
    - 256
  value_obs_key: proprioceptive
training:
  action_repeat: 1
  augment_pixels: false
  batch_size: 256
  deterministic_eval: false
  episode_length: 1000
  log_training_metrics: false
  normalize_observations: true
  num_envs: 8192
  num_eval_envs: 8192
  num_evals: 10
  num_minibatches: 32
  num_resets_per_eval: 0
  num_timesteps: 100000000
  num_updates_per_batch: 4
  optimizer:
    learning_rate: 0.0004
    max_grad_norm: null
  rl_hyperparams:
    clipping_epsilon: 0.3
    discounting: 0.97
    entropy_cost: 0.01
    gae_lambda: 0.95
    normalize_advantage: true
    reward_scaling: 1
  seed: 0
  training_class: PPO
  training_metrics_steps: null
  unroll_length: 20
  use_vision: false
