environment:
  command: {}
  control:
    action_scale: 0.3
  domain_rand:
    apply_kicks: false
    kick_interval: 10
    kick_vel: 0.05
  environment_class: QuadrupedColorGuided
  observation_noise:
    clip: 100.0
    general_noise: 0.05
    history_length: 15
  rewards:
    reward_clip_max: 10000.0
    reward_clip_min: -100.0
    scales:
      action_rate: -0.01
      ang_vel_xy: -0.05
      feet_air_time: 0.2
      foot_slip: -0.1
      goal_progress: 2.5
      goal_yaw_alignment: 1.0
      lin_vel_z: -2.0
      orientation: -5.0
      speed_towards_goal: 2.5
      termination: -1.0
      torques: -0.0002
    termination_body_height: 0.18
    yaw_alignment_threshold: 1.35
  sim:
    ctrl_dt: 0.02
    override:
      Kd: 0.5
      Kp: 35.0
    sim_dt: 0.004
  vision_env_config:
    brightness:
    - 0.75
    - 2.0
    camera_inputs:
    - name: frontal_ego
      use_actual_rgb: false
      use_brightness_randomized_rgb: true
      use_depth: false
    - name: terrain
      use_actual_rgb: true
      use_brightness_randomized_rgb: false
      use_depth: false
    vision_config:
      enabled_cameras:
      - 1
      - 2
      enabled_geom_groups:
      - 0
      - 1
      - 2
      gpu_id: 0
      render_batch_size: 256
      render_height: 64
      render_width: 64
      use_rasterizer: false
model:
  model_class: ActorCritic
  policy:
    layer_sizes:
    - 128
    - 128
    - 128
    - 128
    - 128
  policy_obs_key: proprioceptive
  value:
    layer_sizes:
    - 256
    - 256
    - 256
    - 256
    - 256
  value_obs_key: proprioceptive
terrain:
  base_scene_file: scene_mjx_empty_arena.xml
  goal_location:
  - 10
  - 0
  - 0.5
  goal_size: 0.5
  n_columns: 20
  n_rows: 20
  num_colors: 2
  randomization_config:
    num_colors: 2
    tile_body_prefix: tile_
  square_size: 1.0
  terrain_class: ColorMap
training:
  action_repeat: 1
  augment_pixels: false
  batch_size: 256
  deterministic_eval: false
  episode_length: 1000
  log_training_metrics: false
  normalize_observations: true
  num_envs: 4096
  num_eval_envs: 4096
  num_evals: 10
  num_minibatches: 32
  num_resets_per_eval: 0
  num_timesteps: 100000000
  num_updates_per_batch: 4
  optimizer:
    learning_rate: 0.0004
    max_grad_norm: null
  rl_hyperparams:
    clipping_epsilon: 0.3
    discounting: 0.97
    entropy_cost: 0.01
    gae_lambda: 0.95
    normalize_advantage: true
    reward_scaling: 1
  seed: 0
  training_class: PPO
  training_metrics_steps: null
  unroll_length: 20
  use_vision: false
