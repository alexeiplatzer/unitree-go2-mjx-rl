{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ50qVUYOWsA"
   },
   "source": [
    "# Mujoco RL Renderer\n",
    "This notebook is the rendering part of the mujoco RL pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRN_PG-ol5gn"
   },
   "source": [
    "## Common Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0lObgN8AoJJW"
   },
   "source": [
    "### Install packages, clone repo"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Rt_yq6UaT3P1",
    "outputId": "2c78a8ea-ca30-4331-c1da-9e6896c4c685"
   },
   "source": [
    "# drive_path = '/content/drive/MyDrive/ThesisLab'\n",
    "\n",
    "# !pip install mujoco mujoco_mjx brax mediapy --no-index \\\n",
    "# --find-links $drive_path/mypackages \\\n",
    "# --target /content/mypackages\n",
    "\n",
    "# Install MuJoCo, MJX, and Brax\n",
    "!pip install mujoco\n",
    "!pip install mujoco_mjx\n",
    "!pip install brax"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H4a29MvFnV3U",
    "outputId": "efb1df8b-b781-40a8-ec9e-b5b065087076"
   },
   "source": [
    "# Install mediapy\n",
    "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "!pip install -q mediapy"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaK2NkU0oowB",
    "outputId": "3e336acf-773a-46d0-b670-0bd8caae59f7"
   },
   "source": [
    "# Clone the menagerie\n",
    "!git clone https://github.com/google-deepmind/mujoco_menagerie.git"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWMEpar9mkGE"
   },
   "source": [
    "### Setup output directory and GPU rendering"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jPVJhJu2pt_w"
   },
   "source": [
    "media_dir = '/content/media'\n",
    "!mkdir -p $media_dir"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "g9wuI4bVqQOU"
   },
   "source": [
    "# Set up GPU rendering.\n",
    "from google.colab import files\n",
    "import distutils.util\n",
    "import os\n",
    "import subprocess\n",
    "if subprocess.run('nvidia-smi').returncode:\n",
    "  raise RuntimeError(\n",
    "      'Cannot communicate with GPU. '\n",
    "      'Make sure you are using a GPU Colab runtime. '\n",
    "      'Go to the Runtime menu and select Choose runtime type.')\n",
    "\n",
    "# Add an ICD config so that glvnd can pick up the Nvidia EGL driver.\n",
    "# This is usually installed as part of an Nvidia driver package, but the Colab\n",
    "# kernel doesn't install its driver via APT, and as a result the ICD is missing.\n",
    "# (https://github.com/NVIDIA/libglvnd/blob/master/src/EGL/icd_enumeration.md)\n",
    "NVIDIA_ICD_CONFIG_PATH = '/usr/share/glvnd/egl_vendor.d/10_nvidia.json'\n",
    "if not os.path.exists(NVIDIA_ICD_CONFIG_PATH):\n",
    "  with open(NVIDIA_ICD_CONFIG_PATH, 'w') as f:\n",
    "    f.write(\"\"\"{\n",
    "    \"file_format_version\" : \"1.0.0\",\n",
    "    \"ICD\" : {\n",
    "        \"library_path\" : \"libEGL_nvidia.so.0\"\n",
    "    }\n",
    "}\n",
    "\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8J682gAFqAdL",
    "outputId": "01c0ce56-1be7-4254-f3a4-049cfcc17e9e"
   },
   "source": [
    "# Configure MuJoCo to use the EGL rendering backend (requires GPU)\n",
    "print('Setting environment variable to use GPU rendering:')\n",
    "%env MUJOCO_GL=egl"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E85N9xxAokC0"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tozOzuxTOUXr"
   },
   "source": [
    "# Import all necessary packages\n",
    "\n",
    "# Supporting\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.8\"  # 0.9 causes too much lag.\n",
    "import time\n",
    "import itertools\n",
    "import functools\n",
    "from datetime import datetime\n",
    "from etils import epath\n",
    "from typing import Any, Dict, Sequence, Tuple, Callable, NamedTuple, Optional, Union, List\n",
    "from ml_collections import config_dict\n",
    "import matplotlib.pyplot as plt\n",
    "import mediapy as media\n",
    "\n",
    "# Math\n",
    "import jax\n",
    "import jax.numpy as jp\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3, suppress=True, linewidth=100)  # More legible printing from numpy.\n",
    "from jax import config  # Analytical gradients work much better with double precision.\n",
    "# config.update(\"jax_debug_nans\", True)\n",
    "# config.update(\"jax_enable_x64\", True)\n",
    "# config.update('jax_default_matmul_precision', 'high')\n",
    "from flax.training import orbax_utils\n",
    "from flax import struct\n",
    "from orbax import checkpoint as ocp\n",
    "\n",
    "# Sim\n",
    "import mujoco\n",
    "import mujoco.mjx as mjx\n",
    "\n",
    "# Brax\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Base, Motion, Transform\n",
    "from brax.base import State as PipelineState\n",
    "from brax.envs.base import Env, PipelineEnv, State\n",
    "from brax.mjx.base import State as MjxState\n",
    "from brax.mjx.pipeline import _reformat_contact\n",
    "from brax.training.acme import running_statistics\n",
    "from brax.io import html, mjcf, model\n",
    "\n",
    "# Algorithms\n",
    "from brax.training.agents.apg import train as apg\n",
    "from brax.training.agents.apg import networks as apg_networks\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "svJVgFvoPSi-",
    "outputId": "3b4b0079-4e89-4bd8-d43f-7661a32e5fd0"
   },
   "source": [
    "# Checking that mujoco installation succeeded\n",
    "mujoco.MjModel.from_xml_string(xml='<mujoco/>')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpcyXmtWfpPS"
   },
   "source": [
    "### Render Unitree Go2 initial standing scene"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "_njtIfbqPZtv",
    "outputId": "33f80b01-703d-4111-898a-a3103a159aab"
   },
   "source": [
    "menagerie_path = '/content/mujoco_menagerie'\n",
    "xml_path = epath.Path(menagerie_path + '/unitree_go2/scene_mjx.xml').as_posix()\n",
    "\n",
    "mj_model = mujoco.MjModel.from_xml_path(xml_path)\n",
    "\n",
    "if 'renderer' not in dir():\n",
    "    renderer = mujoco.Renderer(mj_model)\n",
    "\n",
    "init_q = mj_model.keyframe('home').qpos\n",
    "\n",
    "mj_data = mujoco.MjData(mj_model)\n",
    "mj_data.qpos = init_q\n",
    "mujoco.mj_forward(mj_model, mj_data)\n",
    "\n",
    "renderer.update_scene(mj_data)\n",
    "image = renderer.render()\n",
    "media.write_image('/content/media/standing.png', image)\n",
    "media.show_image(image)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j68fHPxNsGYC"
   },
   "source": [
    "### Rollout Rendering function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nI6fCdn7sVHo"
   },
   "source": [
    "# Rendering Rollouts\n",
    "def render_rollout(title, reset_fn, step_fn,\n",
    "                   inference_fn, env,\n",
    "                   n_steps=500, camera=None,\n",
    "                   the_command=None,\n",
    "                   seed=0):\n",
    "    rng = jax.random.key(seed)\n",
    "    render_every = 2\n",
    "    state = reset_fn(rng)\n",
    "    if the_command is not None:\n",
    "      state.info['command'] = the_command\n",
    "    rollout = [state.pipeline_state]\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        act_rng, rng = jax.random.split(rng)\n",
    "        ctrl, _ = inference_fn(state.obs, act_rng)\n",
    "        state = step_fn(state, ctrl)\n",
    "        if i % render_every == 0:\n",
    "            rollout.append(state.pipeline_state)\n",
    "\n",
    "    rendering = env.render(rollout, camera=camera)\n",
    "    media.show_video(env.render(rollout, camera=camera),\n",
    "                     fps=1.0 / (env.dt * render_every),\n",
    "                     codec='gif')\n",
    "    media.write_video(f'{media_dir}/{title}.gif',\n",
    "                     env.render(rollout, camera=camera),\n",
    "                     fps=1.0 / (env.dt * render_every),\n",
    "                     codec='gif')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu4jwZPqmxzp"
   },
   "source": [
    "## Unitree Go2 APG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o52G3ciBuV66"
   },
   "source": [
    "### Reference Kinematics"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RmjeYIWxvao5"
   },
   "source": [
    "def cos_wave(t, step_period, scale):\n",
    "    _cos_wave = -jp.cos(((2 * jp.pi) / step_period) * t)\n",
    "    return _cos_wave * (scale / 2) + (scale / 2)\n",
    "\n",
    "\n",
    "def dcos_wave(t, step_period, scale):\n",
    "    \"\"\"\n",
    "    Derivative of the cos wave, for reference velocity\n",
    "    \"\"\"\n",
    "    return ((scale * jp.pi) / step_period) * jp.sin(((2 * jp.pi) / step_period) * t)\n",
    "\n",
    "\n",
    "def make_kinematic_ref(sinusoid, step_k, scale=0.3, dt=1 / 50):\n",
    "    \"\"\"\n",
    "    Makes trotting kinematics for the 12 leg joints.\n",
    "    step_k is the number of timesteps it takes to raise and lower a given foot.\n",
    "    A gait cycle is 2 * step_k * dt seconds long.\n",
    "    \"\"\"\n",
    "\n",
    "    _steps = jp.arange(step_k)\n",
    "    step_period = step_k * dt\n",
    "    t = _steps * dt\n",
    "\n",
    "    wave = sinusoid(t, step_period, scale)\n",
    "    # Commands for one step of an active front leg\n",
    "    fleg_cmd_block = jp.concatenate(\n",
    "        [jp.zeros((step_k, 1)),\n",
    "         wave.reshape(step_k, 1),\n",
    "         -2 * wave.reshape(step_k, 1)],\n",
    "        axis=1\n",
    "    )\n",
    "    # Our standing config reverses front and hind legs\n",
    "    h_leg_cmd_bloc = 1 * fleg_cmd_block\n",
    "\n",
    "    block1 = jp.concatenate([\n",
    "        jp.zeros((step_k, 3)),\n",
    "        fleg_cmd_block,\n",
    "        h_leg_cmd_bloc,\n",
    "        jp.zeros((step_k, 3))],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    block2 = jp.concatenate([\n",
    "        fleg_cmd_block,\n",
    "        jp.zeros((step_k, 3)),\n",
    "        jp.zeros((step_k, 3)),\n",
    "        h_leg_cmd_bloc],\n",
    "        axis=1\n",
    "    )\n",
    "    # In one step cycle, both pairs of active legs have inactive and active phases\n",
    "    step_cycle = jp.concatenate([block1, block2], axis=0)\n",
    "    return step_cycle"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "XWiD_bDduZcJ",
    "outputId": "bb23f68e-62cc-4065-dba9-f91a09bd105e"
   },
   "source": [
    "poses = make_kinematic_ref(cos_wave, step_k=25)\n",
    "\n",
    "frames = []\n",
    "init_q = mj_model.keyframe('home').qpos\n",
    "mj_data.qpos = init_q\n",
    "default_ap = init_q[7:]\n",
    "\n",
    "for i in range(len(poses)):\n",
    "    mj_data.qpos[7:] = poses[i] + default_ap\n",
    "    mujoco.mj_forward(mj_model, mj_data)\n",
    "    renderer.update_scene(mj_data)\n",
    "    frames.append(renderer.render())\n",
    "\n",
    "media.show_video(frames, fps=50, codec='gif')\n",
    "media.write_video(f'{media_dir}/stepping.gif', frames, fps=50, codec='gif')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGBIVRGK56cy"
   },
   "source": [
    "### In-place Trotting Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "L-He6kmx6rH4"
   },
   "source": [
    "def get_config():\n",
    "    def get_default_rewards_config():\n",
    "        default_config = config_dict.ConfigDict(\n",
    "            dict(\n",
    "                scales=config_dict.ConfigDict(\n",
    "                    dict(\n",
    "                        min_reference_tracking=-2.5 * 3e-3,  # to equalize the magnitude\n",
    "                        reference_tracking=-1.0,\n",
    "                        feet_height=-1.0\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        return default_config\n",
    "\n",
    "    default_config = config_dict.ConfigDict(\n",
    "        dict(rewards=get_default_rewards_config(), ))\n",
    "\n",
    "    return default_config\n",
    "\n",
    "\n",
    "# Math functions from (https://github.com/jiawei-ren/diffmimic)\n",
    "def quaternion_to_matrix(quaternions):\n",
    "    r, i, j, k = quaternions[..., 0], quaternions[..., 1], quaternions[..., 2], quaternions[..., 3]\n",
    "    two_s = 2.0 / (quaternions * quaternions).sum(-1)\n",
    "\n",
    "    o = jp.stack(\n",
    "        (\n",
    "            1 - two_s * (j * j + k * k),\n",
    "            two_s * (i * j - k * r),\n",
    "            two_s * (i * k + j * r),\n",
    "            two_s * (i * j + k * r),\n",
    "            1 - two_s * (i * i + k * k),\n",
    "            two_s * (j * k - i * r),\n",
    "            two_s * (i * k - j * r),\n",
    "            two_s * (j * k + i * r),\n",
    "            1 - two_s * (i * i + j * j),\n",
    "        ),\n",
    "        -1,\n",
    "    )\n",
    "    return o.reshape(quaternions.shape[:-1] + (3, 3))\n",
    "\n",
    "\n",
    "def matrix_to_rotation_6d(matrix):\n",
    "    batch_dim = matrix.shape[:-2]\n",
    "    return matrix[..., :2, :].reshape(batch_dim + (6,))\n",
    "\n",
    "\n",
    "def quaternion_to_rotation_6d(quaternion):\n",
    "    return matrix_to_rotation_6d(quaternion_to_matrix(quaternion))\n",
    "\n",
    "\n",
    "class TrotGo2(PipelineEnv):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            termination_height: float = 0.25,\n",
    "            **kwargs,\n",
    "    ):\n",
    "        step_k = kwargs.pop('step_k', 25)\n",
    "\n",
    "        physics_steps_per_control_step = 10\n",
    "        kwargs['n_frames'] = kwargs.get(\n",
    "            'n_frames', physics_steps_per_control_step)\n",
    "\n",
    "        mj_model = mujoco.MjModel.from_xml_path(xml_path)\n",
    "        kp = 230\n",
    "        mj_model.actuator_gainprm[:, 0] = kp\n",
    "        mj_model.actuator_biasprm[:, 1] = -kp\n",
    "\n",
    "        sys = mjcf.load_model(mj_model)\n",
    "\n",
    "        super().__init__(sys=sys, **kwargs)\n",
    "\n",
    "        self.termination_height = termination_height\n",
    "\n",
    "        self._init_q = mj_model.keyframe('home').qpos\n",
    "\n",
    "        self.err_threshold = 0.4  # diffmimic; value from paper.\n",
    "\n",
    "        self._default_ap_pose = mj_model.keyframe('home').qpos[7:]\n",
    "        self.reward_config = get_config()\n",
    "\n",
    "        self.action_loc = self._default_ap_pose\n",
    "        self.action_scale = jp.array([0.2, 0.8, 0.8] * 4)\n",
    "\n",
    "        self.feet_inds = jp.array([21, 28, 35, 42])  # LF, RF, LH, RH\n",
    "\n",
    "        #### Imitation reference\n",
    "        kinematic_ref_qpos = make_kinematic_ref(\n",
    "            cos_wave, step_k, scale=0.3, dt=self.dt)\n",
    "        kinematic_ref_qvel = make_kinematic_ref(\n",
    "            dcos_wave, step_k, scale=0.3, dt=self.dt)\n",
    "\n",
    "        self.l_cycle = jp.array(kinematic_ref_qpos.shape[0])\n",
    "\n",
    "        # Expand to entire state space.\n",
    "\n",
    "        kinematic_ref_qpos += self._default_ap_pose\n",
    "        ref_qs = np.tile(self._init_q.reshape(1, 19), (self.l_cycle, 1))\n",
    "        ref_qs[:, 7:] = kinematic_ref_qpos\n",
    "        self.kinematic_ref_qpos = jp.array(ref_qs)\n",
    "\n",
    "        ref_qvels = np.zeros((self.l_cycle, 18))\n",
    "        ref_qvels[:, 6:] = kinematic_ref_qvel\n",
    "        self.kinematic_ref_qvel = jp.array(ref_qvels)\n",
    "\n",
    "        # Can decrease jit time and training wall-clock time significantly.\n",
    "        self.pipeline_step = jax.checkpoint(self.pipeline_step,\n",
    "                                            policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\n",
    "\n",
    "    def reset(self, rng: jax.Array) -> State:\n",
    "        # Deterministic init\n",
    "\n",
    "        qpos = jp.array(self._init_q)\n",
    "        qvel = jp.zeros(18)\n",
    "\n",
    "        data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "        # Position onto ground\n",
    "        pen = jp.min(data.contact.dist)\n",
    "        qpos = qpos.at[2].set(qpos[2] - pen)\n",
    "        data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "        state_info = {\n",
    "            'rng': rng,\n",
    "            'steps': 0.0,\n",
    "            'reward_tuple': {\n",
    "                'reference_tracking': 0.0,\n",
    "                'min_reference_tracking': 0.0,\n",
    "                'feet_height': 0.0\n",
    "            },\n",
    "            'last_action': jp.zeros(12),  # from MJX tutorial.\n",
    "            'kinematic_ref': jp.zeros(19),\n",
    "        }\n",
    "\n",
    "        x, xd = data.x, data.xd\n",
    "        obs = self._get_obs(data.qpos, x, xd, state_info)\n",
    "        reward, done = jp.zeros(2)\n",
    "        metrics = {}\n",
    "        for k in state_info['reward_tuple']:\n",
    "            metrics[k] = state_info['reward_tuple'][k]\n",
    "        state = State(data, obs, reward, done, metrics, state_info)\n",
    "        return jax.lax.stop_gradient(state)\n",
    "\n",
    "    def step(self, state: State, action: jax.Array) -> State:\n",
    "        action = jp.clip(action, -1, 1)  # Raw action\n",
    "\n",
    "        action = self.action_loc + (action * self.action_scale)\n",
    "\n",
    "        data = self.pipeline_step(state.pipeline_state, action)\n",
    "\n",
    "        ref_qpos = self.kinematic_ref_qpos[jp.array(state.info['steps'] % self.l_cycle, int)]\n",
    "        ref_qvel = self.kinematic_ref_qvel[jp.array(state.info['steps'] % self.l_cycle, int)]\n",
    "\n",
    "        # Calculate maximal coordinates\n",
    "        ref_data = data.replace(qpos=ref_qpos, qvel=ref_qvel)\n",
    "        ref_data = mjx.forward(self.sys, ref_data)\n",
    "        ref_x, ref_xd = ref_data.x, ref_data.xd\n",
    "\n",
    "        state.info['kinematic_ref'] = ref_qpos\n",
    "\n",
    "        # observation data\n",
    "        x, xd = data.x, data.xd\n",
    "        obs = self._get_obs(data.qpos, x, xd, state.info)\n",
    "\n",
    "        # Terminate if flipped over or fallen down.\n",
    "        done = 0.0\n",
    "        done = jp.where(x.pos[0, 2] < self.termination_height, 1.0, done)\n",
    "        up = jp.array([0.0, 0.0, 1.0])\n",
    "        done = jp.where(jp.dot(math.rotate(up, x.rot[0]), up) < 0, 1.0, done)\n",
    "\n",
    "        # reward\n",
    "        reward_tuple = {\n",
    "            'reference_tracking': (\n",
    "                    self._reward_reference_tracking(x, xd, ref_x, ref_xd)\n",
    "                    * self.reward_config.rewards.scales.reference_tracking\n",
    "            ),\n",
    "            'min_reference_tracking': (\n",
    "                    self._reward_min_reference_tracking(ref_qpos, ref_qvel, state)\n",
    "                    * self.reward_config.rewards.scales.min_reference_tracking\n",
    "            ),\n",
    "            'feet_height': (\n",
    "                    self._reward_feet_height(data.geom_xpos[self.feet_inds][:, 2]\n",
    "                                             , ref_data.geom_xpos[self.feet_inds][:, 2])\n",
    "                    * self.reward_config.rewards.scales.feet_height\n",
    "            )\n",
    "        }\n",
    "\n",
    "        reward = sum(reward_tuple.values())\n",
    "\n",
    "        # state management\n",
    "        state.info['reward_tuple'] = reward_tuple\n",
    "        state.info['last_action'] = action  # used for observation.\n",
    "\n",
    "        for k in state.info['reward_tuple'].keys():\n",
    "            state.metrics[k] = state.info['reward_tuple'][k]\n",
    "\n",
    "        state = state.replace(\n",
    "            pipeline_state=data, obs=obs, reward=reward,\n",
    "            done=done)\n",
    "\n",
    "        #### Reset state to reference if it gets too far\n",
    "        error = (((x.pos - ref_x.pos) ** 2).sum(-1) ** 0.5).mean()\n",
    "        to_reference = jp.where(error > self.err_threshold, 1.0, 0.0)\n",
    "\n",
    "        to_reference = jp.array(to_reference, dtype=int)  # keeps output types same as input.\n",
    "        ref_data = self.mjx_to_brax(ref_data)\n",
    "\n",
    "        data = jax.tree_util.tree_map(lambda x, y:\n",
    "                                      jp.array((1 - to_reference) * x + to_reference * y, x.dtype), data, ref_data)\n",
    "\n",
    "        x, xd = data.x, data.xd  # Data may have changed.\n",
    "        obs = self._get_obs(data.qpos, x, xd, state.info)\n",
    "\n",
    "        return state.replace(pipeline_state=data, obs=obs)\n",
    "\n",
    "    def _get_obs(self, qpos: jax.Array, x: Transform, xd: Motion,\n",
    "                 state_info: Dict[str, Any]) -> jax.Array:\n",
    "\n",
    "        inv_base_orientation = math.quat_inv(x.rot[0])\n",
    "        local_rpyrate = math.rotate(xd.ang[0], inv_base_orientation)\n",
    "\n",
    "        obs_list = []\n",
    "        # yaw rate\n",
    "        obs_list.append(jp.array([local_rpyrate[2]]) * 0.25)\n",
    "        # projected gravity\n",
    "        obs_list.append(\n",
    "            math.rotate(jp.array([0.0, 0.0, -1.0]), inv_base_orientation))\n",
    "        # motor angles\n",
    "        angles = qpos[7:19]\n",
    "        obs_list.append(angles - self._default_ap_pose)\n",
    "        # last action\n",
    "        obs_list.append(state_info['last_action'])\n",
    "        # kinematic reference\n",
    "        kin_ref = self.kinematic_ref_qpos[jp.array(state_info['steps'] % self.l_cycle, int)]\n",
    "        obs_list.append(kin_ref[7:])  # First 7 indicies are fixed\n",
    "\n",
    "        obs = jp.clip(jp.concatenate(obs_list), -100.0, 100.0)\n",
    "\n",
    "        return obs\n",
    "\n",
    "    def mjx_to_brax(self, data):\n",
    "        \"\"\"\n",
    "        Apply the brax wrapper on the core MJX data structure.\n",
    "        \"\"\"\n",
    "        q, qd = data.qpos, data.qvel\n",
    "        x = Transform(pos=data.xpos[1:], rot=data.xquat[1:])\n",
    "        cvel = Motion(vel=data.cvel[1:, 3:], ang=data.cvel[1:, :3])\n",
    "        offset = data.xpos[1:, :] - data.subtree_com[self.sys.body_rootid[1:]]\n",
    "        offset = Transform.create(pos=offset)\n",
    "        xd = offset.vmap().do(cvel)\n",
    "        data = _reformat_contact(self.sys, data)\n",
    "        return data.replace(q=q, qd=qd, x=x, xd=xd)\n",
    "\n",
    "    # ------------ reward functions----------------\n",
    "    def _reward_reference_tracking(self, x, xd, ref_x, ref_xd):\n",
    "        \"\"\"\n",
    "        Rewards based on inertial-frame body positions.\n",
    "        Notably, we use a high-dimension representation of orientation.\n",
    "        \"\"\"\n",
    "\n",
    "        f = lambda x, y: ((x - y) ** 2).sum(-1).mean()\n",
    "\n",
    "        _mse_pos = f(x.pos, ref_x.pos)\n",
    "        _mse_rot = f(quaternion_to_rotation_6d(x.rot),\n",
    "                     quaternion_to_rotation_6d(ref_x.rot))\n",
    "        _mse_vel = f(xd.vel, ref_xd.vel)\n",
    "        _mse_ang = f(xd.ang, ref_xd.ang)\n",
    "\n",
    "        # Tuned to be about the same size.\n",
    "        return _mse_pos \\\n",
    "            + 0.1 * _mse_rot \\\n",
    "            + 0.01 * _mse_vel \\\n",
    "            + 0.001 * _mse_ang\n",
    "\n",
    "    def _reward_min_reference_tracking(self, ref_qpos, ref_qvel, state):\n",
    "        \"\"\"\n",
    "        Using minimal coordinates. Improves accuracy of joint angle tracking.\n",
    "        \"\"\"\n",
    "        pos = jp.concatenate([\n",
    "            state.pipeline_state.qpos[:3],\n",
    "            state.pipeline_state.qpos[7:]])\n",
    "        pos_targ = jp.concatenate([\n",
    "            ref_qpos[:3],\n",
    "            ref_qpos[7:]])\n",
    "        pos_err = jp.linalg.norm(pos_targ - pos)\n",
    "        vel_err = jp.linalg.norm(state.pipeline_state.qvel - ref_qvel)\n",
    "\n",
    "        return pos_err + vel_err\n",
    "\n",
    "    def _reward_feet_height(self, feet_pos, feet_pos_ref):\n",
    "        return jp.sum(jp.abs(feet_pos - feet_pos_ref))  # try to drive it to 0 using the l1 norm.\n",
    "\n",
    "\n",
    "envs.register_environment('trotting_go2', TrotGo2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TbVF1ME33RjV"
   },
   "source": [
    "### Policy Rollout"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "biqvlKGw5pI9"
   },
   "source": [
    "# Each foot contacts the ground twice/sec.\n",
    "env = envs.get_environment(\"trotting_go2\", step_k=13)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p6B4nToeaAsl"
   },
   "source": [
    "# Reconstruct the trotting inference function\n",
    "make_networks_factory = functools.partial(\n",
    "    apg_networks.make_apg_networks,\n",
    "    hidden_layer_sizes=(256, 128)\n",
    ")\n",
    "\n",
    "nets = make_networks_factory(observation_size=1, # Observation_size argument doesn't matter since it's only used for param init.\n",
    "                             action_size=12,\n",
    "                             preprocess_observations_fn=running_statistics.normalize)\n",
    "\n",
    "make_inference_fn = apg_networks.make_inference_fn(nets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "INNew1jM3TtU"
   },
   "source": [
    "model_path = '/content/trotting_2hz_policy'\n",
    "params = model.load_params(model_path)\n",
    "\n",
    "# Inference function for in-place trotting\n",
    "baseline_inference_fn = make_inference_fn(params)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "ZIUHgB5U5L4a",
    "outputId": "44bd8bd9-cd43-4972-8635-42b709af7413"
   },
   "source": [
    "demo_env = envs.training.EpisodeWrapper(env,\n",
    "                                        episode_length=1000,\n",
    "                                        action_repeat=1)\n",
    "\n",
    "render_rollout(\n",
    "  \"go2_trotting\",\n",
    "  jax.jit(demo_env.reset),\n",
    "  jax.jit(demo_env.step),\n",
    "  jax.jit(baseline_inference_fn),\n",
    "  demo_env,\n",
    "  n_steps=200,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16IS7YwF3UH1"
   },
   "source": [
    "### Forward Motion Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jsXpq-Mu3X-N"
   },
   "source": [
    "def axis_angle_to_quaternion(v: jp.ndarray, theta:jp.float_):\n",
    "    \"\"\"\n",
    "    axis angle representation: rotation of theta around v.\n",
    "    \"\"\"\n",
    "    return jp.concatenate([jp.cos(0.5*theta).reshape(1), jp.sin(0.5*theta)*v.reshape(3)])\n",
    "\n",
    "def get_config():\n",
    "  \"\"\"Returns reward config for go2 quadruped environment.\"\"\"\n",
    "\n",
    "  def get_default_rewards_config():\n",
    "    default_config = config_dict.ConfigDict(\n",
    "        dict(\n",
    "            scales=config_dict.ConfigDict(\n",
    "                dict(\n",
    "                    tracking_lin_vel = 1.0,\n",
    "                    orientation = -1.0, # non-flat base\n",
    "                    height = 0.5,\n",
    "                    lin_vel_z=-1.0, # prevents the suicide policy\n",
    "                    torque = -0.01,\n",
    "                    feet_pos = -1, # Bad action hard-coding.\n",
    "                    feet_height = -1, # prevents it from just standing still\n",
    "                    joint_velocity = -0.001\n",
    "                    )\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    "    return default_config\n",
    "\n",
    "  default_config = config_dict.ConfigDict(\n",
    "      dict(rewards=get_default_rewards_config(),))\n",
    "\n",
    "  return default_config\n",
    "\n",
    "class FwdTrotGo2(PipelineEnv):\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      termination_height: float=0.25,\n",
    "      **kwargs,\n",
    "  ):\n",
    "\n",
    "    self.target_vel = kwargs.pop('target_vel', 0.75)\n",
    "    step_k = kwargs.pop('step_k', 25)\n",
    "    self.baseline_inference_fn = kwargs.pop(\"baseline_inference_fn\")\n",
    "    physics_steps_per_control_step = 10\n",
    "    kwargs['n_frames'] = kwargs.get(\n",
    "        'n_frames', physics_steps_per_control_step)\n",
    "    self.termination_height = termination_height\n",
    "\n",
    "    mj_model = mujoco.MjModel.from_xml_path(xml_path)\n",
    "    kp = 230\n",
    "    mj_model.actuator_gainprm[:, 0] = kp\n",
    "    mj_model.actuator_biasprm[:, 1] = -kp\n",
    "    self._init_q = mj_model.keyframe('home').qpos\n",
    "    self._default_ap_pose = mj_model.keyframe('home').qpos[7:]\n",
    "    self.reward_config = get_config()\n",
    "\n",
    "    self.action_loc = self._default_ap_pose\n",
    "    self.action_scale = jp.array([0.2, 0.8, 0.8] * 4)\n",
    "\n",
    "    self.target_h = self._init_q[2]\n",
    "\n",
    "    sys = mjcf.load_model(mj_model)\n",
    "    super().__init__(sys=sys, **kwargs)\n",
    "\n",
    "    \"\"\"\n",
    "    Kinematic references are used for gait scheduling.\n",
    "    \"\"\"\n",
    "\n",
    "    kinematic_ref_qpos = make_kinematic_ref(\n",
    "      cos_wave, step_k, scale=0.3, dt=self.dt)\n",
    "    self.l_cycle = jp.array(kinematic_ref_qpos.shape[0])\n",
    "    self.kinematic_ref_qpos = jp.array(kinematic_ref_qpos + self._default_ap_pose)\n",
    "\n",
    "    \"\"\"\n",
    "    Foot tracking\n",
    "    \"\"\"\n",
    "    gait_k = step_k * 2\n",
    "    self.gait_period = gait_k * self.dt\n",
    "\n",
    "    self.step_k = step_k\n",
    "    self.feet_inds = jp.array([21,28,35,42]) # LF, RF, LH, RH\n",
    "    self.hip_inds = self.feet_inds - 6\n",
    "\n",
    "    self.pipeline_step = jax.checkpoint(self.pipeline_step,\n",
    "      policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\n",
    "\n",
    "  def reset(self, rng: jax.Array) -> State:\n",
    "    rng, key_xyz, key_ang, key_ax, key_q, key_qd = jax.random.split(rng, 6)\n",
    "\n",
    "    qpos = jp.array(self._init_q)\n",
    "    qvel = jp.zeros(18)\n",
    "\n",
    "    #### Add Randomness ####\n",
    "\n",
    "    r_xyz = 0.2 * (jax.random.uniform(key_xyz, (3,))-0.5)\n",
    "    r_angle = (jp.pi/12) * (jax.random.uniform(key_ang, (1,)) - 0.5) # 15 deg range\n",
    "    r_axis = (jax.random.uniform(key_ax, (3,)) - 0.5)\n",
    "    r_axis = r_axis / jp.linalg.norm(r_axis)\n",
    "    r_quat = axis_angle_to_quaternion(r_axis, r_angle)\n",
    "\n",
    "    r_joint_q = 0.2 * (jax.random.uniform(key_q, (12,)) - 0.5)\n",
    "    r_joint_qd = 0.1 * (jax.random.uniform(key_qd, (12,)) - 0.5)\n",
    "\n",
    "    qpos = qpos.at[0:3].set(qpos[0:3] + r_xyz)\n",
    "    qpos = qpos.at[3:7].set(r_quat)\n",
    "    qpos = qpos.at[7:19].set(qpos[7:19] + r_joint_q)\n",
    "    qvel = qvel.at[6:18].set(qvel[6:18] + r_joint_qd)\n",
    "\n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    # Ensure you're not sunken into the ground nor above it.\n",
    "    pen = jp.min(data.contact.dist)\n",
    "    qpos = qpos.at[2].set(qpos[2] - pen)\n",
    "    data = self.pipeline_init(qpos, qvel)\n",
    "\n",
    "    state_info = {\n",
    "        'rng': rng,\n",
    "        'steps': 0.0,\n",
    "        'reward_tuple': {\n",
    "            'tracking_lin_vel': 0.0,\n",
    "            'orientation': 0.0,\n",
    "            'height': 0.0,\n",
    "            'lin_vel_z': 0.0,\n",
    "            'torque': 0.0,\n",
    "            'joint_velocity': 0.0,\n",
    "            'feet_pos': 0.0,\n",
    "            'feet_height': 0.0\n",
    "        },\n",
    "        'last_action': jp.zeros(12), # from MJX tutorial.\n",
    "        'baseline_action': jp.zeros(12),\n",
    "        'xy0': jp.zeros((4, 2)),\n",
    "        'k0': 0.0,\n",
    "        'xy*': jp.zeros((4, 2))\n",
    "    }\n",
    "\n",
    "    x, xd = data.x, data.xd\n",
    "    _obs = self._get_obs(data.qpos, x, xd, state_info) # inner obs; to trotter\n",
    "\n",
    "    action_key, key = jax.random.split(state_info['rng'])\n",
    "    state_info['rng'] = key\n",
    "    next_action, _ = self.baseline_inference_fn(_obs, action_key)\n",
    "\n",
    "    obs = jp.concatenate([_obs, next_action])\n",
    "\n",
    "    reward, done = jp.zeros(2)\n",
    "    metrics = {}\n",
    "    for k in state_info['reward_tuple']:\n",
    "      metrics[k] = state_info['reward_tuple'][k]\n",
    "    state = State(data, obs, reward, done, metrics, state_info)\n",
    "    return jax.lax.stop_gradient(state)\n",
    "\n",
    "  def step(self, state: State, action: jax.Array) -> State:\n",
    "\n",
    "    action = jp.clip(action, -1, 1)\n",
    "\n",
    "    cur_base = state.obs[-12:]\n",
    "    action += cur_base\n",
    "    state.info['baseline_action'] = cur_base\n",
    "\n",
    "    action = self.action_loc + (action * self.action_scale)\n",
    "\n",
    "    data = self.pipeline_step(state.pipeline_state, action)\n",
    "\n",
    "    # observation data\n",
    "    x, xd = data.x, data.xd\n",
    "    obs = self._get_obs(data.qpos, x, xd, state.info)\n",
    "\n",
    "    # Terminate if flipped over or fallen down.\n",
    "    done = 0.0\n",
    "    done = jp.where(x.pos[0, 2] < self.termination_height, 1.0, done)\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    done = jp.where(jp.dot(math.rotate(up, x.rot[0]), up) < 0, 1.0, done)\n",
    "\n",
    "    #### Foot Position Reference Updating ####\n",
    "\n",
    "    # Detect the start of a new step\n",
    "    s = state.info['steps']\n",
    "    step_num = s // (self.step_k)\n",
    "    even_step = step_num % 2 == 0\n",
    "    new_step = (s % self.step_k) == 0\n",
    "    new_even_step = jp.logical_and(new_step, even_step)\n",
    "    new_odd_step = jp.logical_and(new_step, jp.logical_not(even_step))\n",
    "\n",
    "    # Apply railbert heuristic to calculate target foot position, after step\n",
    "    hip_xy = data.geom_xpos[self.hip_inds][:,:2] # 4 x 2\n",
    "    v_body = data.qvel[0:2]\n",
    "    step_period = self.gait_period/2\n",
    "    raibert_xy = hip_xy + (step_period/2) * v_body\n",
    "\n",
    "    # Update.\n",
    "    cur_tars = state.info['xy*']\n",
    "    i_RFLH = jp.array([1, 2])\n",
    "    i_LFRH = jp.array([0, 3])\n",
    "    feet_xy = data.geom_xpos[self.feet_inds][:,:2]\n",
    "\n",
    "    # With the trotting gait, we will move one pair of opposite legs,\n",
    "    # and keep the other pair fixed in place.\n",
    "    case_c1 = raibert_xy.at[i_LFRH].set(feet_xy[i_LFRH])\n",
    "    case_c2 = raibert_xy.at[i_RFLH].set(feet_xy[i_RFLH])\n",
    "    xy_tars = jp.where(new_even_step, case_c1, cur_tars)\n",
    "    xy_tars = jp.where(new_odd_step, case_c2, xy_tars)\n",
    "    state.info['xy*'] = xy_tars\n",
    "\n",
    "    # Save timestep and location at start of step.\n",
    "    state.info['k0'] = jp.where(new_step,\n",
    "                                state.info['steps'],\n",
    "                                state.info['k0'])\n",
    "    state.info['xy0'] = jp.where(new_step,\n",
    "                                 feet_xy,\n",
    "                                 state.info['xy0'])\n",
    "\n",
    "    # reward\n",
    "    reward_tuple = {\n",
    "        'tracking_lin_vel': (\n",
    "            self._reward_tracking_lin_vel(jp.array([self.target_vel, 0, 0]), x, xd)\n",
    "            * self.reward_config.rewards.scales.tracking_lin_vel\n",
    "        ),\n",
    "        'orientation': (\n",
    "          self._reward_orientation(x)\n",
    "          * self.reward_config.rewards.scales.orientation\n",
    "        ),\n",
    "        'lin_vel_z': (\n",
    "            self._reward_lin_vel_z(xd)\n",
    "            * self.reward_config.rewards.scales.lin_vel_z\n",
    "        ),\n",
    "        'height': (\n",
    "          self._reward_height(data.qpos)\n",
    "          * self.reward_config.rewards.scales.height\n",
    "        ),\n",
    "        'torque': (\n",
    "          self._reward_action(data.qfrc_actuator)\n",
    "          * self.reward_config.rewards.scales.torque\n",
    "        ),\n",
    "        'joint_velocity': (\n",
    "          self._reward_joint_velocity(data.qvel)\n",
    "          * self.reward_config.rewards.scales.joint_velocity\n",
    "        ),\n",
    "        'feet_pos': (\n",
    "          self._reward_feet_pos(data, state)\n",
    "          * self.reward_config.rewards.scales.feet_pos\n",
    "        ),\n",
    "        'feet_height': (\n",
    "          self._reward_feet_height(data, state.info)\n",
    "          * self.reward_config.rewards.scales.feet_height\n",
    "        )\n",
    "    }\n",
    "\n",
    "    reward = sum(reward_tuple.values())\n",
    "\n",
    "    # state management\n",
    "    state.info['reward_tuple'] = reward_tuple\n",
    "    state.info['last_action'] = action\n",
    "\n",
    "    for k in state.info['reward_tuple'].keys():\n",
    "      state.metrics[k] = state.info['reward_tuple'][k]\n",
    "\n",
    "    # next action\n",
    "    action_key, key = jax.random.split(state.info['rng'])\n",
    "    state.info['rng'] = key\n",
    "    next_action, _ = self.baseline_inference_fn(obs, action_key)\n",
    "    obs = jp.concatenate([obs, next_action])\n",
    "\n",
    "    state = state.replace(\n",
    "        pipeline_state=data, obs=obs, reward=reward,\n",
    "        done=done)\n",
    "    return state\n",
    "\n",
    "  def _get_obs(self, qpos: jax.Array, x: Transform, xd: Motion,\n",
    "               state_info: Dict[str, Any]) -> jax.Array:\n",
    "\n",
    "    inv_base_orientation = math.quat_inv(x.rot[0])\n",
    "    local_rpyrate = math.rotate(xd.ang[0], inv_base_orientation)\n",
    "\n",
    "    obs_list = []\n",
    "    # yaw rate\n",
    "    obs_list.append(jp.array([local_rpyrate[2]]) * 0.25)\n",
    "    # projected gravity\n",
    "    obs_list.append(\n",
    "        math.rotate(jp.array([0.0, 0.0, -1.0]), inv_base_orientation))\n",
    "    # motor angles\n",
    "    angles = qpos[7:19]\n",
    "    obs_list.append(angles - self._default_ap_pose)\n",
    "    # last action\n",
    "    obs_list.append(state_info['last_action'])\n",
    "    # gait schedule\n",
    "    kin_ref = self.kinematic_ref_qpos[jp.array(state_info['steps']%self.l_cycle, int)]\n",
    "    obs_list.append(kin_ref)\n",
    "\n",
    "    obs = jp.clip(jp.concatenate(obs_list), -100.0, 100.0)\n",
    "\n",
    "    return obs\n",
    "\n",
    "  # ------------ reward functions----------------\n",
    "  def _reward_tracking_lin_vel(\n",
    "      self, commands: jax.Array, x: Transform, xd: Motion) -> jax.Array:\n",
    "    # Tracking of linear velocity commands (xy axes)\n",
    "    local_vel = math.rotate(xd.vel[0], math.quat_inv(x.rot[0]))\n",
    "    lin_vel_error = jp.sum(jp.square(commands[:2] - local_vel[:2]))\n",
    "    lin_vel_reward = jp.exp(-lin_vel_error)\n",
    "    return lin_vel_reward\n",
    "  def _reward_orientation(self, x: Transform) -> jax.Array:\n",
    "    # Penalize non flat base orientation\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    rot_up = math.rotate(up, x.rot[0])\n",
    "    return jp.sum(jp.square(rot_up[:2]))\n",
    "  def _reward_lin_vel_z(self, xd: Motion) -> jax.Array:\n",
    "    # Penalize z axis base linear velocity\n",
    "    return jp.clip(jp.square(xd.vel[0, 2]), 0, 10)\n",
    "  def _reward_joint_velocity(self, qvel):\n",
    "      return jp.clip(jp.sqrt(jp.sum(jp.square(qvel[6:]))), 0, 100)\n",
    "  def _reward_height(self, qpos) -> jax.Array:\n",
    "    return jp.exp(-jp.abs(qpos[2] - self.target_h)) # Not going to be > 1 meter tall.\n",
    "  def _reward_action(self, action) -> jax.Array:\n",
    "    return jp.sqrt(jp.sum(jp.square(action)))\n",
    "  def _reward_feet_pos(self, data, state):\n",
    "    dt = (state.info['steps'] - state.info['k0']) * self.dt # scalar\n",
    "    step_period = self.gait_period / 2\n",
    "    xyt = state.info['xy0'] + (state.info['xy*'] - state.info['xy0']) * (dt/step_period)\n",
    "\n",
    "    feet_pos = data.geom_xpos[self.feet_inds][:, :2]\n",
    "\n",
    "    rews = jp.sum(jp.square(feet_pos - xyt), axis=1)\n",
    "    rews = jp.clip(rews, 0, 10)\n",
    "    return jp.sum(rews)\n",
    "  def _reward_feet_height(self, data, state_info):\n",
    "    \"\"\"\n",
    "    Feet height tracks rectified sine waves\n",
    "    \"\"\"\n",
    "    h_tar = 0.1\n",
    "    t = state_info['steps'] * self.dt\n",
    "    offset = self.gait_period/2\n",
    "    ref1 = jp.sin((2*jp.pi/self.gait_period)*t) # RF and LH feet\n",
    "    ref2 = jp.sin((2*jp.pi/self.gait_period)*(t - offset)) # LF and RH\n",
    "\n",
    "    ref1, ref2 = ref1 * h_tar, ref2 * h_tar\n",
    "    h_tars = jp.array([ref2, ref1, ref1, ref2])\n",
    "    h_tars = h_tars.clip(min=0, max=None) + 0.02 # offset height of feet.\n",
    "\n",
    "    feet_height = data.geom_xpos[self.feet_inds][:,2]\n",
    "    errs = jp.clip(jp.square(feet_height - h_tars), 0, 10)\n",
    "    return jp.sum(errs)\n",
    "\n",
    "envs.register_environment('go2', FwdTrotGo2)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIj_6L5x3BfL"
   },
   "source": [
    "### Forward Motion Rollout"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9lhbhTI_3Eur"
   },
   "source": [
    "# Construct Environment\n",
    "env_kwargs = dict(target_vel=0.75, step_k=13,\n",
    "                  baseline_inference_fn=baseline_inference_fn)\n",
    "\n",
    "env = envs.get_environment(\"go2\", **env_kwargs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hzWRnaIT4feu"
   },
   "source": [
    "# Reconstruct the locomotion inference function\n",
    "make_networks_factory = functools.partial(\n",
    "    apg_networks.make_apg_networks,\n",
    "    hidden_layer_sizes=(128, 64)\n",
    ")\n",
    "\n",
    "nets = make_networks_factory(observation_size=1, # Observation_size argument doesn't matter since it's only used for param init.\n",
    "                             action_size=12,\n",
    "                             preprocess_observations_fn=running_statistics.normalize)\n",
    "\n",
    "make_inference_fn = apg_networks.make_inference_fn(nets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "e3nW6c9A_0SN"
   },
   "source": [
    "model_path = '/content/running_2hz_policy'\n",
    "params = model.load_params(model_path)\n",
    "\n",
    "# Inference function for in-place trotting\n",
    "locomotion_inference_fn = make_inference_fn(params)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "oGldRYQi__aB",
    "outputId": "476c50ab-9e54-499b-c1cd-95afb71bb8f4"
   },
   "source": [
    "demo_env = envs.training.EpisodeWrapper(env,\n",
    "                                        episode_length=1000,\n",
    "                                        action_repeat=1)\n",
    "\n",
    "render_rollout(\n",
    "  \"forward_trot\",\n",
    "  jax.jit(demo_env.reset),\n",
    "  jax.jit(demo_env.step),\n",
    "  jax.jit(locomotion_inference_fn),\n",
    "  demo_env,\n",
    "  n_steps=200,\n",
    "  camera=\"track\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVcL6pzKatft"
   },
   "source": [
    "## Unitree Go2 PPO"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4xt2vJxLhRro",
    "cellView": "form"
   },
   "source": [
    "#@title Unitree Go2 Quadruped Env\n",
    "menagerie_path = '/content/mujoco_menagerie'\n",
    "GO2_ROOT_PATH = epath.Path(menagerie_path + '/unitree_go2')\n",
    "\n",
    "\n",
    "def get_config():\n",
    "  \"\"\"Returns reward config for barkour quadruped environment.\"\"\"\n",
    "\n",
    "  def get_default_rewards_config():\n",
    "    default_config = config_dict.ConfigDict(\n",
    "        dict(\n",
    "            # The coefficients for all reward terms used for training. All\n",
    "            # physical quantities are in SI units, if no otherwise specified,\n",
    "            # i.e. joint positions are in rad, positions are measured in meters,\n",
    "            # torques in Nm, and time in seconds, and forces in Newtons.\n",
    "            scales=config_dict.ConfigDict(\n",
    "                dict(\n",
    "                    # Tracking rewards are computed using exp(-delta^2/sigma)\n",
    "                    # sigma can be a hyperparameters to tune.\n",
    "                    # Track the base x-y velocity (no z-velocity tracking.)\n",
    "                    tracking_lin_vel=1.5,\n",
    "                    # Track the angular velocity along z-axis, i.e. yaw rate.\n",
    "                    tracking_ang_vel=0.8,\n",
    "                    # Below are regularization terms, we roughly divide the\n",
    "                    # terms to base state regularizations, joint\n",
    "                    # regularizations, and other behavior regularizations.\n",
    "                    # Penalize the base velocity in z direction, L2 penalty.\n",
    "                    lin_vel_z=-2.0,\n",
    "                    # Penalize the base roll and pitch rate. L2 penalty.\n",
    "                    ang_vel_xy=-0.05,\n",
    "                    # Penalize non-zero roll and pitch angles. L2 penalty.\n",
    "                    orientation=-5.0,\n",
    "                    # L2 regularization of joint torques, |tau|^2.\n",
    "                    torques=-0.0002,\n",
    "                    # Penalize the change in the action and encourage smooth\n",
    "                    # actions. L2 regularization |action - last_action|^2\n",
    "                    action_rate=-0.01,\n",
    "                    # Encourage long swing steps.  However, it does not\n",
    "                    # encourage high clearances.\n",
    "                    feet_air_time=0.2,\n",
    "                    # Encourage no motion at zero command, L2 regularization\n",
    "                    # |q - q_default|^2.\n",
    "                    stand_still=-0.5,\n",
    "                    # Early termination penalty.\n",
    "                    termination=-1.0,\n",
    "                    # Penalizing foot slipping on the ground.\n",
    "                    foot_slip=-0.1,\n",
    "                )\n",
    "            ),\n",
    "            # Tracking reward = exp(-error^2/sigma).\n",
    "            tracking_sigma=0.25,\n",
    "        )\n",
    "    )\n",
    "    return default_config\n",
    "\n",
    "  default_config = config_dict.ConfigDict(\n",
    "      dict(\n",
    "          rewards=get_default_rewards_config(),\n",
    "      )\n",
    "  )\n",
    "\n",
    "  return default_config\n",
    "\n",
    "\n",
    "class Go2JoystickEnv(PipelineEnv):\n",
    "  \"\"\"Environment for training the go2 quadruped joystick policy in MJX.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      obs_noise: float = 0.05,\n",
    "      action_scale: float = 0.3,\n",
    "      kick_vel: float = 0.05,\n",
    "      scene_file: str = 'scene_mjx.xml',\n",
    "      **kwargs,\n",
    "  ):\n",
    "    path = GO2_ROOT_PATH / scene_file\n",
    "    sys = mjcf.load(path.as_posix())\n",
    "    self._dt = 0.02  # this environment is 50 fps\n",
    "    sys = sys.tree_replace({'opt.timestep': 0.004})\n",
    "\n",
    "    # override menagerie params for smoother policy\n",
    "    sys = sys.replace(\n",
    "        dof_damping=sys.dof_damping.at[6:].set(0.5239),\n",
    "        actuator_gainprm=sys.actuator_gainprm.at[:, 0].set(35.0),\n",
    "        actuator_biasprm=sys.actuator_biasprm.at[:, 1].set(-35.0),\n",
    "    )\n",
    "\n",
    "    n_frames = kwargs.pop('n_frames', int(self._dt / sys.opt.timestep))\n",
    "    super().__init__(sys, backend='mjx', n_frames=n_frames)\n",
    "\n",
    "    self.reward_config = get_config()\n",
    "    # set custom from kwargs\n",
    "    for k, v in kwargs.items():\n",
    "      if k.endswith('_scale'):\n",
    "        self.reward_config.rewards.scales[k[:-6]] = v\n",
    "\n",
    "    self._torso_idx = mujoco.mj_name2id(\n",
    "        sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, 'base'\n",
    "    )\n",
    "    self._action_scale = action_scale\n",
    "    self._obs_noise = obs_noise\n",
    "    self._kick_vel = kick_vel\n",
    "    self._init_q = jp.array(sys.mj_model.keyframe('home').qpos)\n",
    "    self._default_pose = sys.mj_model.keyframe('home').qpos[7:]\n",
    "    self.lowers = jp.array([-0.7, -1.0, -2.3] * 4)\n",
    "    self.uppers = jp.array([0.52, 2.1, -1] * 4)\n",
    "    feet_site = [\n",
    "        'FL_foot',\n",
    "        'RL_foot',\n",
    "        'FR_foot',\n",
    "        'RR_foot',\n",
    "    ]\n",
    "    feet_site_id = [\n",
    "        mujoco.mj_name2id(sys.mj_model, mujoco.mjtObj.mjOBJ_SITE.value, f)\n",
    "        for f in feet_site\n",
    "    ]\n",
    "    assert not any(id_ == -1 for id_ in feet_site_id), 'Site not found.'\n",
    "    self._feet_site_id = np.array(feet_site_id)\n",
    "    lower_leg_body = [\n",
    "        'FL_calf',\n",
    "        'RL_calf',\n",
    "        'FR_calf',\n",
    "        'RR_calf',\n",
    "    ]\n",
    "    lower_leg_body_id = [\n",
    "        mujoco.mj_name2id(sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, l)\n",
    "        for l in lower_leg_body\n",
    "    ]\n",
    "    assert not any(id_ == -1 for id_ in lower_leg_body_id), 'Body not found.'\n",
    "    self._lower_leg_body_id = np.array(lower_leg_body_id)\n",
    "    self._foot_radius = 0.0175\n",
    "    self._nv = sys.nv\n",
    "\n",
    "  def sample_command(self, rng: jax.Array) -> jax.Array:\n",
    "    lin_vel_x = [-0.6, 1.5]  # min max [m/s]\n",
    "    lin_vel_y = [-0.8, 0.8]  # min max [m/s]\n",
    "    ang_vel_yaw = [-0.7, 0.7]  # min max [rad/s]\n",
    "\n",
    "    _, key1, key2, key3 = jax.random.split(rng, 4)\n",
    "    lin_vel_x = jax.random.uniform(\n",
    "        key1, (1,), minval=lin_vel_x[0], maxval=lin_vel_x[1]\n",
    "    )\n",
    "    lin_vel_y = jax.random.uniform(\n",
    "        key2, (1,), minval=lin_vel_y[0], maxval=lin_vel_y[1]\n",
    "    )\n",
    "    ang_vel_yaw = jax.random.uniform(\n",
    "        key3, (1,), minval=ang_vel_yaw[0], maxval=ang_vel_yaw[1]\n",
    "    )\n",
    "    new_cmd = jp.array([lin_vel_x[0], lin_vel_y[0], ang_vel_yaw[0]])\n",
    "    return new_cmd\n",
    "\n",
    "  def reset(self, rng: jax.Array) -> State:  # pytype: disable=signature-mismatch\n",
    "    rng, key = jax.random.split(rng)\n",
    "\n",
    "    pipeline_state = self.pipeline_init(self._init_q, jp.zeros(self._nv))\n",
    "\n",
    "    state_info = {\n",
    "        'rng': rng,\n",
    "        'last_act': jp.zeros(12),\n",
    "        'last_vel': jp.zeros(12),\n",
    "        'command': self.sample_command(key),\n",
    "        'last_contact': jp.zeros(4, dtype=bool),\n",
    "        'feet_air_time': jp.zeros(4),\n",
    "        'rewards': {k: 0.0 for k in self.reward_config.rewards.scales.keys()},\n",
    "        'kick': jp.array([0.0, 0.0]),\n",
    "        'step': 0,\n",
    "    }\n",
    "\n",
    "    obs_history = jp.zeros(15 * 31)  # store 15 steps of history\n",
    "    obs = self._get_obs(pipeline_state, state_info, obs_history)\n",
    "    reward, done = jp.zeros(2)\n",
    "    metrics = {'total_dist': 0.0}\n",
    "    for k in state_info['rewards']:\n",
    "      metrics[k] = state_info['rewards'][k]\n",
    "    state = State(pipeline_state, obs, reward, done, metrics, state_info)  # pytype: disable=wrong-arg-types\n",
    "    return state\n",
    "\n",
    "  def step(self, state: State, action: jax.Array) -> State:  # pytype: disable=signature-mismatch\n",
    "    rng, cmd_rng, kick_noise_2 = jax.random.split(state.info['rng'], 3)\n",
    "\n",
    "    # kick\n",
    "    push_interval = 10\n",
    "    kick_theta = jax.random.uniform(kick_noise_2, maxval=2 * jp.pi)\n",
    "    kick = jp.array([jp.cos(kick_theta), jp.sin(kick_theta)])\n",
    "    kick *= jp.mod(state.info['step'], push_interval) == 0\n",
    "    qvel = state.pipeline_state.qvel  # pytype: disable=attribute-error\n",
    "    qvel = qvel.at[:2].set(kick * self._kick_vel + qvel[:2])\n",
    "    state = state.tree_replace({'pipeline_state.qvel': qvel})\n",
    "\n",
    "    # physics step\n",
    "    motor_targets = self._default_pose + action * self._action_scale\n",
    "    motor_targets = jp.clip(motor_targets, self.lowers, self.uppers)\n",
    "    pipeline_state = self.pipeline_step(state.pipeline_state, motor_targets)\n",
    "    x, xd = pipeline_state.x, pipeline_state.xd\n",
    "\n",
    "    # observation data\n",
    "    obs = self._get_obs(pipeline_state, state.info, state.obs)\n",
    "    joint_angles = pipeline_state.q[7:]\n",
    "    joint_vel = pipeline_state.qd[6:]\n",
    "\n",
    "    # foot contact data based on z-position\n",
    "    foot_pos = pipeline_state.site_xpos[self._feet_site_id]  # pytype: disable=attribute-error\n",
    "    foot_contact_z = foot_pos[:, 2] - self._foot_radius\n",
    "    contact = foot_contact_z < 1e-3  # a mm or less off the floor\n",
    "    contact_filt_mm = contact | state.info['last_contact']\n",
    "    contact_filt_cm = (foot_contact_z < 3e-2) | state.info['last_contact']\n",
    "    first_contact = (state.info['feet_air_time'] > 0) * contact_filt_mm\n",
    "    state.info['feet_air_time'] += self.dt\n",
    "\n",
    "    # done if joint limits are reached or robot is falling\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    done = jp.dot(math.rotate(up, x.rot[self._torso_idx - 1]), up) < 0\n",
    "    done |= jp.any(joint_angles < self.lowers)\n",
    "    done |= jp.any(joint_angles > self.uppers)\n",
    "    done |= pipeline_state.x.pos[self._torso_idx - 1, 2] < 0.18\n",
    "\n",
    "    # reward\n",
    "    rewards = {\n",
    "        'tracking_lin_vel': (\n",
    "            self._reward_tracking_lin_vel(state.info['command'], x, xd)\n",
    "        ),\n",
    "        'tracking_ang_vel': (\n",
    "            self._reward_tracking_ang_vel(state.info['command'], x, xd)\n",
    "        ),\n",
    "        'lin_vel_z': self._reward_lin_vel_z(xd),\n",
    "        'ang_vel_xy': self._reward_ang_vel_xy(xd),\n",
    "        'orientation': self._reward_orientation(x),\n",
    "        'torques': self._reward_torques(pipeline_state.qfrc_actuator),  # pytype: disable=attribute-error\n",
    "        'action_rate': self._reward_action_rate(action, state.info['last_act']),\n",
    "        'stand_still': self._reward_stand_still(\n",
    "            state.info['command'], joint_angles,\n",
    "        ),\n",
    "        'feet_air_time': self._reward_feet_air_time(\n",
    "            state.info['feet_air_time'],\n",
    "            first_contact,\n",
    "            state.info['command'],\n",
    "        ),\n",
    "        'foot_slip': self._reward_foot_slip(pipeline_state, contact_filt_cm),\n",
    "        'termination': self._reward_termination(done, state.info['step']),\n",
    "    }\n",
    "    rewards = {\n",
    "        k: v * self.reward_config.rewards.scales[k] for k, v in rewards.items()\n",
    "    }\n",
    "    reward = jp.clip(sum(rewards.values()) * self.dt, 0.0, 10000.0)\n",
    "\n",
    "    # state management\n",
    "    state.info['kick'] = kick\n",
    "    state.info['last_act'] = action\n",
    "    state.info['last_vel'] = joint_vel\n",
    "    state.info['feet_air_time'] *= ~contact_filt_mm\n",
    "    state.info['last_contact'] = contact\n",
    "    state.info['rewards'] = rewards\n",
    "    state.info['step'] += 1\n",
    "    state.info['rng'] = rng\n",
    "\n",
    "    # sample new command if more than 500 timesteps achieved\n",
    "    state.info['command'] = jp.where(\n",
    "        state.info['step'] > 500,\n",
    "        self.sample_command(cmd_rng),\n",
    "        state.info['command'],\n",
    "    )\n",
    "    # reset the step counter when done\n",
    "    state.info['step'] = jp.where(\n",
    "        done | (state.info['step'] > 500), 0, state.info['step']\n",
    "    )\n",
    "\n",
    "    # log total displacement as a proxy metric\n",
    "    state.metrics['total_dist'] = math.normalize(x.pos[self._torso_idx - 1])[1]\n",
    "    state.metrics.update(state.info['rewards'])\n",
    "\n",
    "    done = jp.float32(done)\n",
    "    state = state.replace(\n",
    "        pipeline_state=pipeline_state, obs=obs, reward=reward, done=done\n",
    "    )\n",
    "    return state\n",
    "\n",
    "  def _get_obs(\n",
    "      self,\n",
    "      pipeline_state: base.State,\n",
    "      state_info: dict[str, Any],\n",
    "      obs_history: jax.Array,\n",
    "  ) -> jax.Array:\n",
    "    inv_torso_rot = math.quat_inv(pipeline_state.x.rot[0])\n",
    "    local_rpyrate = math.rotate(pipeline_state.xd.ang[0], inv_torso_rot)\n",
    "\n",
    "    obs = jp.concatenate([\n",
    "        jp.array([local_rpyrate[2]]) * 0.25,                 # yaw rate\n",
    "        math.rotate(jp.array([0, 0, -1]), inv_torso_rot),    # projected gravity\n",
    "        state_info['command'] * jp.array([2.0, 2.0, 0.25]),  # command\n",
    "        pipeline_state.q[7:] - self._default_pose,           # motor angles\n",
    "        state_info['last_act'],                              # last action\n",
    "    ])\n",
    "\n",
    "    # clip, noise\n",
    "    obs = jp.clip(obs, -100.0, 100.0) + self._obs_noise * jax.random.uniform(\n",
    "        state_info['rng'], obs.shape, minval=-1, maxval=1\n",
    "    )\n",
    "    # stack observations through time\n",
    "    obs = jp.roll(obs_history, obs.size).at[:obs.size].set(obs)\n",
    "\n",
    "    return obs\n",
    "\n",
    "  # ------------ reward functions----------------\n",
    "  def _reward_lin_vel_z(self, xd: Motion) -> jax.Array:\n",
    "    # Penalize z axis base linear velocity\n",
    "    return jp.square(xd.vel[0, 2])\n",
    "\n",
    "  def _reward_ang_vel_xy(self, xd: Motion) -> jax.Array:\n",
    "    # Penalize xy axes base angular velocity\n",
    "    return jp.sum(jp.square(xd.ang[0, :2]))\n",
    "\n",
    "  def _reward_orientation(self, x: Transform) -> jax.Array:\n",
    "    # Penalize non flat base orientation\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    rot_up = math.rotate(up, x.rot[0])\n",
    "    return jp.sum(jp.square(rot_up[:2]))\n",
    "\n",
    "  def _reward_torques(self, torques: jax.Array) -> jax.Array:\n",
    "    # Penalize torques\n",
    "    return jp.sqrt(jp.sum(jp.square(torques))) + jp.sum(jp.abs(torques))\n",
    "\n",
    "  def _reward_action_rate(\n",
    "      self, act: jax.Array, last_act: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # Penalize changes in actions\n",
    "    return jp.sum(jp.square(act - last_act))\n",
    "\n",
    "  def _reward_tracking_lin_vel(\n",
    "      self, commands: jax.Array, x: Transform, xd: Motion\n",
    "  ) -> jax.Array:\n",
    "    # Tracking of linear velocity commands (xy axes)\n",
    "    local_vel = math.rotate(xd.vel[0], math.quat_inv(x.rot[0]))\n",
    "    lin_vel_error = jp.sum(jp.square(commands[:2] - local_vel[:2]))\n",
    "    lin_vel_reward = jp.exp(\n",
    "        -lin_vel_error / self.reward_config.rewards.tracking_sigma\n",
    "    )\n",
    "    return lin_vel_reward\n",
    "\n",
    "  def _reward_tracking_ang_vel(\n",
    "      self, commands: jax.Array, x: Transform, xd: Motion\n",
    "  ) -> jax.Array:\n",
    "    # Tracking of angular velocity commands (yaw)\n",
    "    base_ang_vel = math.rotate(xd.ang[0], math.quat_inv(x.rot[0]))\n",
    "    ang_vel_error = jp.square(commands[2] - base_ang_vel[2])\n",
    "    return jp.exp(-ang_vel_error / self.reward_config.rewards.tracking_sigma)\n",
    "\n",
    "  def _reward_feet_air_time(\n",
    "      self, air_time: jax.Array, first_contact: jax.Array, commands: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # Reward air time.\n",
    "    rew_air_time = jp.sum((air_time - 0.1) * first_contact)\n",
    "    rew_air_time *= (\n",
    "        math.normalize(commands[:2])[1] > 0.05\n",
    "    )  # no reward for zero command\n",
    "    return rew_air_time\n",
    "\n",
    "  def _reward_stand_still(\n",
    "      self,\n",
    "      commands: jax.Array,\n",
    "      joint_angles: jax.Array,\n",
    "  ) -> jax.Array:\n",
    "    # Penalize motion at zero commands\n",
    "    return jp.sum(jp.abs(joint_angles - self._default_pose)) * (\n",
    "        math.normalize(commands[:2])[1] < 0.1\n",
    "    )\n",
    "\n",
    "  def _reward_foot_slip(\n",
    "      self, pipeline_state: base.State, contact_filt: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # get velocities at feet which are offset from lower legs\n",
    "    # pytype: disable=attribute-error\n",
    "    pos = pipeline_state.site_xpos[self._feet_site_id]  # feet position\n",
    "    feet_offset = pos - pipeline_state.xpos[self._lower_leg_body_id]\n",
    "    # pytype: enable=attribute-error\n",
    "    offset = base.Transform.create(pos=feet_offset)\n",
    "    foot_indices = self._lower_leg_body_id - 1  # we got rid of the world body\n",
    "    foot_vel = offset.vmap().do(pipeline_state.xd.take(foot_indices)).vel\n",
    "\n",
    "    # Penalize large feet velocity for feet that are in contact with the ground.\n",
    "    return jp.sum(jp.square(foot_vel[:, :2]) * contact_filt.reshape((-1, 1)))\n",
    "\n",
    "  def _reward_termination(self, done: jax.Array, step: jax.Array) -> jax.Array:\n",
    "    return done & (step < 500)\n",
    "\n",
    "  def render(\n",
    "      self, trajectory: List[base.State], camera: str | None = None,\n",
    "      width: int = 240, height: int = 320,\n",
    "  ) -> Sequence[np.ndarray]:\n",
    "    camera = camera or 'track'\n",
    "    return super().render(trajectory, camera=camera, width=width, height=height)\n",
    "\n",
    "envs.register_environment('joystick_go2', Go2JoystickEnv)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MA-yReezfDJ9"
   },
   "source": [
    "### Rollout"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vcwMCXrcfHTv"
   },
   "source": [
    "env_name = 'joystick_go2'\n",
    "env = envs.get_environment(env_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fXJ0cRPiiea0"
   },
   "source": [
    "# Reconstruct the locomotion inference function\n",
    "make_networks_factory = functools.partial(\n",
    "    ppo_networks.make_ppo_networks,\n",
    "    policy_hidden_layer_sizes=(128, 128, 128, 128)\n",
    ")\n",
    "\n",
    "nets = make_networks_factory(observation_size=1, # Observation_size argument doesn't matter since it's only used for param init.\n",
    "                             action_size=12,\n",
    "                             preprocess_observations_fn=running_statistics.normalize)\n",
    "\n",
    "make_inference_fn = apg_networks.make_inference_fn(nets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RLopRO7hjF8I"
   },
   "source": [
    "model_path = '/content/mjx_brax_go2_policy'\n",
    "params = model.load_params(model_path)\n",
    "\n",
    "# Inference function for in-place trotting\n",
    "ppo_inference_fn = make_inference_fn(params)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7o_9hqbOkipO"
   },
   "source": [
    "# @markdown Commands **only used for Barkour Env**:\n",
    "x_vel = 2.0  #@param {type: \"number\"}\n",
    "y_vel = 0.0  #@param {type: \"number\"}\n",
    "ang_vel = 0.0  #@param {type: \"number\"}\n",
    "\n",
    "the_command = jp.array([x_vel, y_vel, ang_vel])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "8wu3NFYwjPC1",
    "outputId": "cfe494e8-2d2c-49bf-b1a4-422f8562535c"
   },
   "source": [
    "demo_env = envs.training.EpisodeWrapper(env,\n",
    "                                        episode_length=1000,\n",
    "                                        action_repeat=1)\n",
    "\n",
    "render_rollout(\n",
    "  \"ppo_run\",\n",
    "  jax.jit(demo_env.reset),\n",
    "  jax.jit(demo_env.step),\n",
    "  jax.jit(ppo_inference_fn),\n",
    "  demo_env,\n",
    "  n_steps=500,\n",
    "  the_command=the_command,\n",
    "  camera=\"track\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDDTnsZJaY0M"
   },
   "source": [
    "## Barkour vb PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m1M_yTjnmTV"
   },
   "source": [
    "### Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LuMUBdbRno8r",
    "outputId": "eaa74bbe-a1fb-4f57-f630-5c76c501af68"
   },
   "source": [
    "#@title Barkour vb Quadruped Env\n",
    "menagerie_path = '/content/mujoco_menagerie'\n",
    "BARKOUR_ROOT_PATH = epath.Path(menagerie_path + '/google_barkour_vb')\n",
    "\n",
    "def get_config():\n",
    "  \"\"\"Returns reward config for barkour quadruped environment.\"\"\"\n",
    "\n",
    "  def get_default_rewards_config():\n",
    "    default_config = config_dict.ConfigDict(\n",
    "        dict(\n",
    "            # The coefficients for all reward terms used for training. All\n",
    "            # physical quantities are in SI units, if no otherwise specified,\n",
    "            # i.e. joint positions are in rad, positions are measured in meters,\n",
    "            # torques in Nm, and time in seconds, and forces in Newtons.\n",
    "            scales=config_dict.ConfigDict(\n",
    "                dict(\n",
    "                    # Tracking rewards are computed using exp(-delta^2/sigma)\n",
    "                    # sigma can be a hyperparameters to tune.\n",
    "                    # Track the base x-y velocity (no z-velocity tracking.)\n",
    "                    tracking_lin_vel=1.5,\n",
    "                    # Track the angular velocity along z-axis, i.e. yaw rate.\n",
    "                    tracking_ang_vel=0.8,\n",
    "                    # Below are regularization terms, we roughly divide the\n",
    "                    # terms to base state regularizations, joint\n",
    "                    # regularizations, and other behavior regularizations.\n",
    "                    # Penalize the base velocity in z direction, L2 penalty.\n",
    "                    lin_vel_z=-2.0,\n",
    "                    # Penalize the base roll and pitch rate. L2 penalty.\n",
    "                    ang_vel_xy=-0.05,\n",
    "                    # Penalize non-zero roll and pitch angles. L2 penalty.\n",
    "                    orientation=-5.0,\n",
    "                    # L2 regularization of joint torques, |tau|^2.\n",
    "                    torques=-0.0002,\n",
    "                    # Penalize the change in the action and encourage smooth\n",
    "                    # actions. L2 regularization |action - last_action|^2\n",
    "                    action_rate=-0.01,\n",
    "                    # Encourage long swing steps.  However, it does not\n",
    "                    # encourage high clearances.\n",
    "                    feet_air_time=0.2,\n",
    "                    # Encourage no motion at zero command, L2 regularization\n",
    "                    # |q - q_default|^2.\n",
    "                    stand_still=-0.5,\n",
    "                    # Early termination penalty.\n",
    "                    termination=-1.0,\n",
    "                    # Penalizing foot slipping on the ground.\n",
    "                    foot_slip=-0.1,\n",
    "                )\n",
    "            ),\n",
    "            # Tracking reward = exp(-error^2/sigma).\n",
    "            tracking_sigma=0.25,\n",
    "        )\n",
    "    )\n",
    "    return default_config\n",
    "\n",
    "  default_config = config_dict.ConfigDict(\n",
    "      dict(\n",
    "          rewards=get_default_rewards_config(),\n",
    "      )\n",
    "  )\n",
    "\n",
    "  return default_config\n",
    "\n",
    "\n",
    "class BarkourEnv(PipelineEnv):\n",
    "  \"\"\"Environment for training the barkour quadruped joystick policy in MJX.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      obs_noise: float = 0.05,\n",
    "      action_scale: float = 0.3,\n",
    "      kick_vel: float = 0.05,\n",
    "      scene_file: str = 'scene_mjx.xml',\n",
    "      **kwargs,\n",
    "  ):\n",
    "    path = BARKOUR_ROOT_PATH / scene_file\n",
    "    sys = mjcf.load(path.as_posix())\n",
    "    self._dt = 0.02  # this environment is 50 fps\n",
    "    sys = sys.tree_replace({'opt.timestep': 0.004})\n",
    "\n",
    "    # override menagerie params for smoother policy\n",
    "    sys = sys.replace(\n",
    "        dof_damping=sys.dof_damping.at[6:].set(0.5239),\n",
    "        actuator_gainprm=sys.actuator_gainprm.at[:, 0].set(35.0),\n",
    "        actuator_biasprm=sys.actuator_biasprm.at[:, 1].set(-35.0),\n",
    "    )\n",
    "\n",
    "    n_frames = kwargs.pop('n_frames', int(self._dt / sys.opt.timestep))\n",
    "    super().__init__(sys, backend='mjx', n_frames=n_frames)\n",
    "\n",
    "    self.reward_config = get_config()\n",
    "    # set custom from kwargs\n",
    "    for k, v in kwargs.items():\n",
    "      if k.endswith('_scale'):\n",
    "        self.reward_config.rewards.scales[k[:-6]] = v\n",
    "\n",
    "    self._torso_idx = mujoco.mj_name2id(\n",
    "        sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, 'torso'\n",
    "    )\n",
    "    self._action_scale = action_scale\n",
    "    self._obs_noise = obs_noise\n",
    "    self._kick_vel = kick_vel\n",
    "    self._init_q = jp.array(sys.mj_model.keyframe('home').qpos)\n",
    "    self._default_pose = sys.mj_model.keyframe('home').qpos[7:]\n",
    "    self.lowers = jp.array([-0.7, -1.0, 0.05] * 4)\n",
    "    self.uppers = jp.array([0.52, 2.1, 2.1] * 4)\n",
    "    feet_site = [\n",
    "        'foot_front_left',\n",
    "        'foot_hind_left',\n",
    "        'foot_front_right',\n",
    "        'foot_hind_right',\n",
    "    ]\n",
    "    feet_site_id = [\n",
    "        mujoco.mj_name2id(sys.mj_model, mujoco.mjtObj.mjOBJ_SITE.value, f)\n",
    "        for f in feet_site\n",
    "    ]\n",
    "    assert not any(id_ == -1 for id_ in feet_site_id), 'Site not found.'\n",
    "    self._feet_site_id = np.array(feet_site_id)\n",
    "    lower_leg_body = [\n",
    "        'lower_leg_front_left',\n",
    "        'lower_leg_hind_left',\n",
    "        'lower_leg_front_right',\n",
    "        'lower_leg_hind_right',\n",
    "    ]\n",
    "    lower_leg_body_id = [\n",
    "        mujoco.mj_name2id(sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, l)\n",
    "        for l in lower_leg_body\n",
    "    ]\n",
    "    assert not any(id_ == -1 for id_ in lower_leg_body_id), 'Body not found.'\n",
    "    self._lower_leg_body_id = np.array(lower_leg_body_id)\n",
    "    self._foot_radius = 0.0175\n",
    "    self._nv = sys.nv\n",
    "\n",
    "  def sample_command(self, rng: jax.Array) -> jax.Array:\n",
    "    lin_vel_x = [-0.6, 1.5]  # min max [m/s]\n",
    "    lin_vel_y = [-0.8, 0.8]  # min max [m/s]\n",
    "    ang_vel_yaw = [-0.7, 0.7]  # min max [rad/s]\n",
    "\n",
    "    _, key1, key2, key3 = jax.random.split(rng, 4)\n",
    "    lin_vel_x = jax.random.uniform(\n",
    "        key1, (1,), minval=lin_vel_x[0], maxval=lin_vel_x[1]\n",
    "    )\n",
    "    lin_vel_y = jax.random.uniform(\n",
    "        key2, (1,), minval=lin_vel_y[0], maxval=lin_vel_y[1]\n",
    "    )\n",
    "    ang_vel_yaw = jax.random.uniform(\n",
    "        key3, (1,), minval=ang_vel_yaw[0], maxval=ang_vel_yaw[1]\n",
    "    )\n",
    "    new_cmd = jp.array([lin_vel_x[0], lin_vel_y[0], ang_vel_yaw[0]])\n",
    "    return new_cmd\n",
    "\n",
    "  def reset(self, rng: jax.Array) -> State:  # pytype: disable=signature-mismatch\n",
    "    rng, key = jax.random.split(rng)\n",
    "\n",
    "    pipeline_state = self.pipeline_init(self._init_q, jp.zeros(self._nv))\n",
    "\n",
    "    state_info = {\n",
    "        'rng': rng,\n",
    "        'last_act': jp.zeros(12),\n",
    "        'last_vel': jp.zeros(12),\n",
    "        'command': self.sample_command(key),\n",
    "        'last_contact': jp.zeros(4, dtype=bool),\n",
    "        'feet_air_time': jp.zeros(4),\n",
    "        'rewards': {k: 0.0 for k in self.reward_config.rewards.scales.keys()},\n",
    "        'kick': jp.array([0.0, 0.0]),\n",
    "        'step': 0,\n",
    "    }\n",
    "\n",
    "    obs_history = jp.zeros(15 * 31)  # store 15 steps of history\n",
    "    obs = self._get_obs(pipeline_state, state_info, obs_history)\n",
    "    reward, done = jp.zeros(2)\n",
    "    metrics = {'total_dist': 0.0}\n",
    "    for k in state_info['rewards']:\n",
    "      metrics[k] = state_info['rewards'][k]\n",
    "    state = State(pipeline_state, obs, reward, done, metrics, state_info)  # pytype: disable=wrong-arg-types\n",
    "    return state\n",
    "\n",
    "  def step(self, state: State, action: jax.Array) -> State:  # pytype: disable=signature-mismatch\n",
    "    rng, cmd_rng, kick_noise_2 = jax.random.split(state.info['rng'], 3)\n",
    "\n",
    "    # kick\n",
    "    push_interval = 10\n",
    "    kick_theta = jax.random.uniform(kick_noise_2, maxval=2 * jp.pi)\n",
    "    kick = jp.array([jp.cos(kick_theta), jp.sin(kick_theta)])\n",
    "    kick *= jp.mod(state.info['step'], push_interval) == 0\n",
    "    qvel = state.pipeline_state.qvel  # pytype: disable=attribute-error\n",
    "    qvel = qvel.at[:2].set(kick * self._kick_vel + qvel[:2])\n",
    "    state = state.tree_replace({'pipeline_state.qvel': qvel})\n",
    "\n",
    "    # physics step\n",
    "    motor_targets = self._default_pose + action * self._action_scale\n",
    "    motor_targets = jp.clip(motor_targets, self.lowers, self.uppers)\n",
    "    pipeline_state = self.pipeline_step(state.pipeline_state, motor_targets)\n",
    "    x, xd = pipeline_state.x, pipeline_state.xd\n",
    "\n",
    "    # observation data\n",
    "    obs = self._get_obs(pipeline_state, state.info, state.obs)\n",
    "    joint_angles = pipeline_state.q[7:]\n",
    "    joint_vel = pipeline_state.qd[6:]\n",
    "\n",
    "    # foot contact data based on z-position\n",
    "    foot_pos = pipeline_state.site_xpos[self._feet_site_id]  # pytype: disable=attribute-error\n",
    "    foot_contact_z = foot_pos[:, 2] - self._foot_radius\n",
    "    contact = foot_contact_z < 1e-3  # a mm or less off the floor\n",
    "    contact_filt_mm = contact | state.info['last_contact']\n",
    "    contact_filt_cm = (foot_contact_z < 3e-2) | state.info['last_contact']\n",
    "    first_contact = (state.info['feet_air_time'] > 0) * contact_filt_mm\n",
    "    state.info['feet_air_time'] += self.dt\n",
    "\n",
    "    # done if joint limits are reached or robot is falling\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    done = jp.dot(math.rotate(up, x.rot[self._torso_idx - 1]), up) < 0\n",
    "    done |= jp.any(joint_angles < self.lowers)\n",
    "    done |= jp.any(joint_angles > self.uppers)\n",
    "    done |= pipeline_state.x.pos[self._torso_idx - 1, 2] < 0.18\n",
    "\n",
    "    # reward\n",
    "    rewards = {\n",
    "        'tracking_lin_vel': (\n",
    "            self._reward_tracking_lin_vel(state.info['command'], x, xd)\n",
    "        ),\n",
    "        'tracking_ang_vel': (\n",
    "            self._reward_tracking_ang_vel(state.info['command'], x, xd)\n",
    "        ),\n",
    "        'lin_vel_z': self._reward_lin_vel_z(xd),\n",
    "        'ang_vel_xy': self._reward_ang_vel_xy(xd),\n",
    "        'orientation': self._reward_orientation(x),\n",
    "        'torques': self._reward_torques(pipeline_state.qfrc_actuator),  # pytype: disable=attribute-error\n",
    "        'action_rate': self._reward_action_rate(action, state.info['last_act']),\n",
    "        'stand_still': self._reward_stand_still(\n",
    "            state.info['command'], joint_angles,\n",
    "        ),\n",
    "        'feet_air_time': self._reward_feet_air_time(\n",
    "            state.info['feet_air_time'],\n",
    "            first_contact,\n",
    "            state.info['command'],\n",
    "        ),\n",
    "        'foot_slip': self._reward_foot_slip(pipeline_state, contact_filt_cm),\n",
    "        'termination': self._reward_termination(done, state.info['step']),\n",
    "    }\n",
    "    rewards = {\n",
    "        k: v * self.reward_config.rewards.scales[k] for k, v in rewards.items()\n",
    "    }\n",
    "    reward = jp.clip(sum(rewards.values()) * self.dt, 0.0, 10000.0)\n",
    "\n",
    "    # state management\n",
    "    state.info['kick'] = kick\n",
    "    state.info['last_act'] = action\n",
    "    state.info['last_vel'] = joint_vel\n",
    "    state.info['feet_air_time'] *= ~contact_filt_mm\n",
    "    state.info['last_contact'] = contact\n",
    "    state.info['rewards'] = rewards\n",
    "    state.info['step'] += 1\n",
    "    state.info['rng'] = rng\n",
    "\n",
    "    # sample new command if more than 500 timesteps achieved\n",
    "    state.info['command'] = jp.where(\n",
    "        state.info['step'] > 500,\n",
    "        self.sample_command(cmd_rng),\n",
    "        state.info['command'],\n",
    "    )\n",
    "    # reset the step counter when done\n",
    "    state.info['step'] = jp.where(\n",
    "        done | (state.info['step'] > 500), 0, state.info['step']\n",
    "    )\n",
    "\n",
    "    # log total displacement as a proxy metric\n",
    "    state.metrics['total_dist'] = math.normalize(x.pos[self._torso_idx - 1])[1]\n",
    "    state.metrics.update(state.info['rewards'])\n",
    "\n",
    "    done = jp.float32(done)\n",
    "    state = state.replace(\n",
    "        pipeline_state=pipeline_state, obs=obs, reward=reward, done=done\n",
    "    )\n",
    "    return state\n",
    "\n",
    "  def _get_obs(\n",
    "      self,\n",
    "      pipeline_state: base.State,\n",
    "      state_info: dict[str, Any],\n",
    "      obs_history: jax.Array,\n",
    "  ) -> jax.Array:\n",
    "    inv_torso_rot = math.quat_inv(pipeline_state.x.rot[0])\n",
    "    local_rpyrate = math.rotate(pipeline_state.xd.ang[0], inv_torso_rot)\n",
    "\n",
    "    obs = jp.concatenate([\n",
    "        jp.array([local_rpyrate[2]]) * 0.25,                 # yaw rate\n",
    "        math.rotate(jp.array([0, 0, -1]), inv_torso_rot),    # projected gravity\n",
    "        state_info['command'] * jp.array([2.0, 2.0, 0.25]),  # command\n",
    "        pipeline_state.q[7:] - self._default_pose,           # motor angles\n",
    "        state_info['last_act'],                              # last action\n",
    "    ])\n",
    "\n",
    "    # clip, noise\n",
    "    obs = jp.clip(obs, -100.0, 100.0) + self._obs_noise * jax.random.uniform(\n",
    "        state_info['rng'], obs.shape, minval=-1, maxval=1\n",
    "    )\n",
    "    # stack observations through time\n",
    "    obs = jp.roll(obs_history, obs.size).at[:obs.size].set(obs)\n",
    "\n",
    "    return obs\n",
    "\n",
    "  # ------------ reward functions----------------\n",
    "  def _reward_lin_vel_z(self, xd: Motion) -> jax.Array:\n",
    "    # Penalize z axis base linear velocity\n",
    "    return jp.square(xd.vel[0, 2])\n",
    "\n",
    "  def _reward_ang_vel_xy(self, xd: Motion) -> jax.Array:\n",
    "    # Penalize xy axes base angular velocity\n",
    "    return jp.sum(jp.square(xd.ang[0, :2]))\n",
    "\n",
    "  def _reward_orientation(self, x: Transform) -> jax.Array:\n",
    "    # Penalize non flat base orientation\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    rot_up = math.rotate(up, x.rot[0])\n",
    "    return jp.sum(jp.square(rot_up[:2]))\n",
    "\n",
    "  def _reward_torques(self, torques: jax.Array) -> jax.Array:\n",
    "    # Penalize torques\n",
    "    return jp.sqrt(jp.sum(jp.square(torques))) + jp.sum(jp.abs(torques))\n",
    "\n",
    "  def _reward_action_rate(\n",
    "      self, act: jax.Array, last_act: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # Penalize changes in actions\n",
    "    return jp.sum(jp.square(act - last_act))\n",
    "\n",
    "  def _reward_tracking_lin_vel(\n",
    "      self, commands: jax.Array, x: Transform, xd: Motion\n",
    "  ) -> jax.Array:\n",
    "    # Tracking of linear velocity commands (xy axes)\n",
    "    local_vel = math.rotate(xd.vel[0], math.quat_inv(x.rot[0]))\n",
    "    lin_vel_error = jp.sum(jp.square(commands[:2] - local_vel[:2]))\n",
    "    lin_vel_reward = jp.exp(\n",
    "        -lin_vel_error / self.reward_config.rewards.tracking_sigma\n",
    "    )\n",
    "    return lin_vel_reward\n",
    "\n",
    "  def _reward_tracking_ang_vel(\n",
    "      self, commands: jax.Array, x: Transform, xd: Motion\n",
    "  ) -> jax.Array:\n",
    "    # Tracking of angular velocity commands (yaw)\n",
    "    base_ang_vel = math.rotate(xd.ang[0], math.quat_inv(x.rot[0]))\n",
    "    ang_vel_error = jp.square(commands[2] - base_ang_vel[2])\n",
    "    return jp.exp(-ang_vel_error / self.reward_config.rewards.tracking_sigma)\n",
    "\n",
    "  def _reward_feet_air_time(\n",
    "      self, air_time: jax.Array, first_contact: jax.Array, commands: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # Reward air time.\n",
    "    rew_air_time = jp.sum((air_time - 0.1) * first_contact)\n",
    "    rew_air_time *= (\n",
    "        math.normalize(commands[:2])[1] > 0.05\n",
    "    )  # no reward for zero command\n",
    "    return rew_air_time\n",
    "\n",
    "  def _reward_stand_still(\n",
    "      self,\n",
    "      commands: jax.Array,\n",
    "      joint_angles: jax.Array,\n",
    "  ) -> jax.Array:\n",
    "    # Penalize motion at zero commands\n",
    "    return jp.sum(jp.abs(joint_angles - self._default_pose)) * (\n",
    "        math.normalize(commands[:2])[1] < 0.1\n",
    "    )\n",
    "\n",
    "  def _reward_foot_slip(\n",
    "      self, pipeline_state: base.State, contact_filt: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # get velocities at feet which are offset from lower legs\n",
    "    # pytype: disable=attribute-error\n",
    "    pos = pipeline_state.site_xpos[self._feet_site_id]  # feet position\n",
    "    feet_offset = pos - pipeline_state.xpos[self._lower_leg_body_id]\n",
    "    # pytype: enable=attribute-error\n",
    "    offset = base.Transform.create(pos=feet_offset)\n",
    "    foot_indices = self._lower_leg_body_id - 1  # we got rid of the world body\n",
    "    foot_vel = offset.vmap().do(pipeline_state.xd.take(foot_indices)).vel\n",
    "\n",
    "    # Penalize large feet velocity for feet that are in contact with the ground.\n",
    "    return jp.sum(jp.square(foot_vel[:, :2]) * contact_filt.reshape((-1, 1)))\n",
    "\n",
    "  def _reward_termination(self, done: jax.Array, step: jax.Array) -> jax.Array:\n",
    "    return done & (step < 500)\n",
    "\n",
    "  def render(\n",
    "      self, trajectory: List[base.State], camera: str | None = None,\n",
    "      width: int = 240, height: int = 320,\n",
    "  ) -> Sequence[np.ndarray]:\n",
    "    camera = camera or 'track'\n",
    "    return super().render(trajectory, camera=camera, width=width, height=height)\n",
    "\n",
    "envs.register_environment('barkour', BarkourEnv)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vD-ine4tok8_"
   },
   "source": [
    "### Rollout"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OQoFN0E1o2gV"
   },
   "source": [
    "env_name = 'barkour'\n",
    "env = envs.get_environment(env_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PSyVvUp1om-z"
   },
   "source": [
    "# Reconstruct the locomotion inference function\n",
    "make_networks_factory = functools.partial(\n",
    "    ppo_networks.make_ppo_networks,\n",
    "    policy_hidden_layer_sizes=(128, 128, 128, 128)\n",
    ")\n",
    "\n",
    "nets = make_networks_factory(observation_size=1, # Observation_size argument doesn't matter since it's only used for param init.\n",
    "                             action_size=12,\n",
    "                             preprocess_observations_fn=running_statistics.normalize)\n",
    "\n",
    "make_inference_fn = apg_networks.make_inference_fn(nets)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pHiQYuxmpJR4"
   },
   "source": [
    "model_path = '/content/mjx_brax_barkour_policy'\n",
    "params = model.load_params(model_path)\n",
    "\n",
    "# Inference function for in-place trotting\n",
    "ppo_inference_fn = make_inference_fn(params)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5RXQPdKDpQ56"
   },
   "source": [
    "# @markdown Commands **only used for Barkour Env**:\n",
    "x_vel = 3.0  #@param {type: \"number\"}\n",
    "y_vel = 0.0  #@param {type: \"number\"}\n",
    "ang_vel = 0.0  #@param {type: \"number\"}\n",
    "\n",
    "the_command = jp.array([x_vel, y_vel, ang_vel])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "9DIoYIu8pR_Z",
    "outputId": "ec287163-ac5d-4426-8a28-565b454b6d52"
   },
   "source": [
    "demo_env = envs.training.EpisodeWrapper(env,\n",
    "                                        episode_length=1000,\n",
    "                                        action_repeat=1)\n",
    "\n",
    "render_rollout(\n",
    "  \"barkour_ppo_trot\",\n",
    "  jax.jit(demo_env.reset),\n",
    "  jax.jit(demo_env.step),\n",
    "  jax.jit(ppo_inference_fn),\n",
    "  demo_env,\n",
    "  n_steps=1000,\n",
    "  the_command=the_command,\n",
    "  camera=\"track\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "16IS7YwF3UH1"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
